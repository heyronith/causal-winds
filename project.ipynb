{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df430ce6",
   "metadata": {},
   "source": [
    "# Causal Flight Cancellation Prediction\n",
    "\n",
    "**Project Goal**: Predict flight cancellations and attribute them to specific causes using causal inference.\n",
    "\n",
    "**Timeline**: 4-6 weeks | **Focus**: Causation, not just correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase 1: Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df55cf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "üì¶ Pandas version: 2.3.3\n",
      "üì¶ PyTorch version: 2.9.1\n",
      "üì¶ DoWhy version: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data collection\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Causal Inference\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "\n",
    "# Attribution\n",
    "import shap\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üì¶ DoWhy version: {dowhy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f3d9b",
   "metadata": {},
   "source": [
    "## Phase 2: Data Collection\n",
    "\n",
    "**Goal**: Collect flight data (BTS), weather data (NOAA), and metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef542572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Data directories created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA COLLECTION: Flight Cancellation Prediction\n",
    "# ============================================================================\n",
    "# Sources:\n",
    "# 1. BTS (Bureau of Transportation Statistics) - Flight data\n",
    "# 2. NOAA - Weather data\n",
    "# 3. OpenFlights - Airport/Airline metadata\n",
    "# ============================================================================\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "os.makedirs(\"data/external\", exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"üìÅ Data directories created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3158b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 539,747 flights from Jan_25.csv\n",
      "‚úÖ Loaded 631,428 flights from July_25.csv\n",
      "‚úÖ Loaded 562,439 flights from SepT_2025.csv\n",
      "\n",
      "üíæ Saved merged data to: data/processed/flights_combined.csv\n",
      "\n",
      "============================================================\n",
      "COMBINED DATA SUMMARY\n",
      "============================================================\n",
      "Total flights: 1,733,614\n",
      "Date range: 2025-01-01 00:00:00 to 2025-09-30 00:00:00\n",
      "Total cancellations: 34,671.0 (2.00%)\n",
      "\n",
      "Cancellations by month:\n",
      "  January: 16,312 cancellations (3.02%)\n",
      "  July: 15,473 cancellations (2.45%)\n",
      "  September: 2,886 cancellations (0.51%)\n",
      "\n",
      "Cancellation reasons (overall):\n",
      "  B (Weather): 25,216 (72.7%)\n",
      "  A (Carrier): 5,379 (15.5%)\n",
      "  C (NAS): 4,068 (11.7%)\n",
      "  D (Security): 8 (0.0%)\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_bts_data_multiple(filepaths, output_path=None):\n",
    "    \"\"\"\n",
    "    Load and merge multiple BTS flight data files.\n",
    "    \n",
    "    Args:\n",
    "        filepaths: List of file paths or single file path\n",
    "        output_path: Optional path to save merged CSV\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame\n",
    "    \"\"\"\n",
    "    # Convert single filepath to list\n",
    "    if isinstance(filepaths, str):\n",
    "        filepaths = [filepaths]\n",
    "    \n",
    "    # Key columns for cancellation analysis\n",
    "    key_cols = [\n",
    "        'FL_DATE', 'OP_CARRIER', 'OP_CARRIER_FL_NUM', \n",
    "        'ORIGIN', 'DEST', 'CANCELLED', 'CANCELLATION_CODE',\n",
    "        'CRS_DEP_TIME', 'DEP_TIME', 'CRS_ARR_TIME', 'ARR_TIME',\n",
    "        'DAY_OF_WEEK', 'MONTH',\n",
    "        'WEATHER_DELAY', 'CARRIER_DELAY', 'NAS_DELAY', \n",
    "        'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY',\n",
    "        'DISTANCE', 'DIVERTED'\n",
    "    ]\n",
    "    \n",
    "    # Load and combine all files\n",
    "    dfs = []\n",
    "    for filepath in filepaths:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        # Select available columns\n",
    "        available_cols = [col for col in key_cols if col in df.columns]\n",
    "        df = df[available_cols].copy()\n",
    "        \n",
    "        # Convert FL_DATE to datetime\n",
    "        if 'FL_DATE' in df.columns:\n",
    "            df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
    "        \n",
    "        dfs.append(df)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} flights from {filepath.split('/')[-1]}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    if 'FL_DATE' in combined_df.columns:\n",
    "        combined_df = combined_df.sort_values('FL_DATE').reset_index(drop=True)\n",
    "    \n",
    "    # Save merged file if output path provided\n",
    "    if output_path:\n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nüíæ Saved merged data to: {output_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMBINED DATA SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total flights: {len(combined_df):,}\")\n",
    "    print(f\"Date range: {combined_df['FL_DATE'].min()} to {combined_df['FL_DATE'].max()}\")\n",
    "    print(f\"Total cancellations: {combined_df['CANCELLED'].sum():,} ({combined_df['CANCELLED'].mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Cancellation by month\n",
    "    if 'MONTH' in combined_df.columns:\n",
    "        print(f\"\\nCancellations by month:\")\n",
    "        month_cancel = combined_df.groupby('MONTH')['CANCELLED'].agg(['sum', 'mean', 'count'])\n",
    "        month_cancel.columns = ['Cancellations', 'Cancel_Rate', 'Total_Flights']\n",
    "        month_cancel['Cancel_Rate'] = month_cancel['Cancel_Rate'] * 100\n",
    "        month_names = {1: 'January', 7: 'July', 9: 'September'}\n",
    "        for month, row in month_cancel.iterrows():\n",
    "            month_name = month_names.get(month, f'Month {month}')\n",
    "            print(f\"  {month_name}: {row['Cancellations']:,.0f} cancellations ({row['Cancel_Rate']:.2f}%)\")\n",
    "    \n",
    "    # Cancellation reasons\n",
    "    if 'CANCELLATION_CODE' in combined_df.columns:\n",
    "        print(f\"\\nCancellation reasons (overall):\")\n",
    "        cancel_reasons = combined_df[combined_df['CANCELLED'] == 1]['CANCELLATION_CODE'].value_counts()\n",
    "        reason_map = {'A': 'Carrier', 'B': 'Weather', 'C': 'NAS', 'D': 'Security'}\n",
    "        for code, count in cancel_reasons.items():\n",
    "            reason = reason_map.get(code, code)\n",
    "            pct = count / combined_df['CANCELLED'].sum() * 100\n",
    "            print(f\"  {code} ({reason}): {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Usage:\n",
    "filepaths = [\n",
    "    \"/Users/ronny/Desktop/ML projects/Flight Cancels/data/raw/Jan_25.csv\",  # Update with your actual filenames\n",
    "    \"/Users/ronny/Desktop/ML projects/Flight Cancels/data/raw/July_25.csv\",\n",
    "    \"/Users/ronny/Desktop/ML projects/Flight Cancels/data/raw/SepT_2025.csv\"\n",
    "]\n",
    "\n",
    "flights_df = load_bts_data_multiple(\n",
    "    filepaths, \n",
    "    output_path=\"data/processed/flights_combined.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6ee85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AIRPORT ANALYSIS\n",
      "============================================================\n",
      "Total unique airports: 348\n",
      "Total flights: 3,467,228\n",
      "\n",
      "üìä Top 50 Airports (by flight volume):\n",
      "   Total flights in top 50: 2,714,667 (78.3% of all flights)\n",
      "\n",
      "Top 50 airports:\n",
      "    1. ORD: 162,019 flights (4.67%)\n",
      "    2. DEN: 161,432 flights (4.66%)\n",
      "    3. DFW: 159,448 flights (4.60%)\n",
      "    4. ATL: 156,550 flights (4.52%)\n",
      "    5. CLT: 97,050 flights (2.80%)\n",
      "    6. LAX: 95,035 flights (2.74%)\n",
      "    7. PHX: 90,919 flights (2.62%)\n",
      "    8. LAS: 90,272 flights (2.60%)\n",
      "    9. SEA: 86,043 flights (2.48%)\n",
      "   10. MCO: 75,773 flights (2.19%)\n",
      "   11. DCA: 71,504 flights (2.06%)\n",
      "   12. SFO: 71,248 flights (2.05%)\n",
      "   13. BOS: 70,268 flights (2.03%)\n",
      "   14. LGA: 67,978 flights (1.96%)\n",
      "   15. EWR: 61,122 flights (1.76%)\n",
      "   16. DTW: 61,023 flights (1.76%)\n",
      "   17. IAH: 58,114 flights (1.68%)\n",
      "   18. SLC: 58,021 flights (1.67%)\n",
      "   19. MSP: 57,216 flights (1.65%)\n",
      "   20. MIA: 53,657 flights (1.55%)\n",
      "   21. JFK: 52,306 flights (1.51%)\n",
      "   22. BNA: 51,091 flights (1.47%)\n",
      "   23. BWI: 47,964 flights (1.38%)\n",
      "   24. PHL: 47,845 flights (1.38%)\n",
      "   25. SAN: 47,679 flights (1.38%)\n",
      "   26. AUS: 41,854 flights (1.21%)\n",
      "   27. FLL: 41,005 flights (1.18%)\n",
      "   28. MDW: 38,361 flights (1.11%)\n",
      "   29. TPA: 37,455 flights (1.08%)\n",
      "   30. DAL: 35,245 flights (1.02%)\n",
      "   31. STL: 31,787 flights (0.92%)\n",
      "   32. PDX: 31,073 flights (0.90%)\n",
      "   33. HNL: 29,650 flights (0.86%)\n",
      "   34. SMF: 28,897 flights (0.83%)\n",
      "   35. IAD: 28,134 flights (0.81%)\n",
      "   36. HOU: 27,328 flights (0.79%)\n",
      "   37. RDU: 27,191 flights (0.78%)\n",
      "   38. MSY: 23,729 flights (0.68%)\n",
      "   39. MCI: 23,702 flights (0.68%)\n",
      "   40. SJC: 23,183 flights (0.67%)\n",
      "   41. IND: 22,902 flights (0.66%)\n",
      "   42. SNA: 22,462 flights (0.65%)\n",
      "   43. CMH: 21,882 flights (0.63%)\n",
      "   44. PIT: 21,400 flights (0.62%)\n",
      "   45. SAT: 20,266 flights (0.58%)\n",
      "   46. CLE: 19,519 flights (0.56%)\n",
      "   47. OAK: 17,259 flights (0.50%)\n",
      "   48. SJU: 17,161 flights (0.49%)\n",
      "   49. CVG: 16,197 flights (0.47%)\n",
      "   50. RSW: 15,448 flights (0.45%)\n",
      "\n",
      "üíæ Saved top 50 to: data/processed/top_50_airports.csv\n",
      "\n",
      "üìã All airports (348 total, sorted by code):\n",
      "['ABE', 'ABI', 'ABQ', 'ABR', 'ABY', 'ACK', 'ACT', 'ACV', 'ACY', 'ADK', 'ADQ', 'AEX', 'AGS', 'AKN', 'ALB', 'ALO', 'AMA', 'ANC', 'APN', 'ASE', 'ATL', 'ATW', 'ATY', 'AUS', 'AVL', 'AVP', 'AZA', 'AZO', 'BDL', 'BET', 'BFF', 'BFL', 'BGM', 'BGR', 'BHM', 'BIH', 'BIL', 'BIS', 'BJI', 'BLI', 'BLV', 'BMI', 'BNA', 'BOI', 'BOS', 'BPT', 'BQN', 'BRD', 'BRO', 'BRW', 'BTM', 'BTR', 'BTV', 'BUF', 'BUR', 'BWI', 'BZN', 'CAE', 'CAK', 'CDC', 'CDV', 'CHA', 'CHO', 'CHS', 'CID', 'CIU', 'CKB', 'CLD', 'CLE', 'CLL', 'CLT', 'CMH', 'CMI', 'CMX', 'COD', 'COS', 'COU', 'CPR', 'CRP', 'CRW', 'CVG', 'CWA', 'CYS', 'DAB', 'DAL', 'DAY', 'DCA', 'DDC', 'DEC', 'DEN', 'DFW', 'DHN', 'DIK', 'DLG', 'DLH', 'DRO', 'DSM', 'DTW', 'DVL', 'EAR', 'EAU', 'ECP', 'EGE', 'EKO', 'ELM', 'ELP', 'ESC', 'EUG', 'EVV', 'EWN', 'EWR', 'EYW', 'FAI', 'FAR', 'FAT', 'FAY', 'FCA', 'FLG', 'FLL', 'FMN', 'FNT', 'FOD', 'FSD', 'FSM', 'FWA', 'GCC', 'GCK', 'GEG', 'GFK', 'GGG', 'GJT', 'GNV', 'GPT', 'GRB', 'GRI', 'GRK', 'GRR', 'GSO', 'GSP', 'GST', 'GTF', 'GTR', 'GUC', 'GUF', 'GUM', 'HDN', 'HGR', 'HHH', 'HIB', 'HLN', 'HNL', 'HOB', 'HOU', 'HPN', 'HRL', 'HSV', 'HTS', 'HYA', 'HYS', 'IAD', 'IAG', 'IAH', 'ICT', 'IDA', 'ILM', 'IMT', 'IND', 'INL', 'ISP', 'ITO', 'JAC', 'JAN', 'JAX', 'JFK', 'JLN', 'JMS', 'JNU', 'JST', 'KOA', 'KTN', 'LAF', 'LAN', 'LAR', 'LAS', 'LAW', 'LAX', 'LBB', 'LBE', 'LBF', 'LBL', 'LCH', 'LCK', 'LEX', 'LFT', 'LGA', 'LGB', 'LIH', 'LIT', 'LNK', 'LRD', 'LSE', 'LWS', 'MAF', 'MBS', 'MCI', 'MCO', 'MCW', 'MDT', 'MDW', 'MEI', 'MEM', 'MFE', 'MFR', 'MGM', 'MGW', 'MHK', 'MHT', 'MIA', 'MKE', 'MLB', 'MLI', 'MLU', 'MOB', 'MOT', 'MQT', 'MRY', 'MSN', 'MSO', 'MSP', 'MSY', 'MTJ', 'MVY', 'MYR', 'OAJ', 'OAK', 'OGG', 'OKC', 'OMA', 'OME', 'ONT', 'ORD', 'ORF', 'ORH', 'OTH', 'OTZ', 'PAE', 'PBG', 'PBI', 'PDX', 'PGD', 'PHL', 'PHX', 'PIA', 'PIB', 'PIE', 'PIH', 'PIR', 'PIT', 'PLN', 'PNS', 'PPG', 'PQI', 'PRC', 'PSC', 'PSE', 'PSG', 'PSM', 'PSP', 'PVD', 'PVU', 'PWM', 'RAP', 'RDD', 'RDM', 'RDU', 'RFD', 'RHI', 'RIC', 'RIW', 'RKS', 'RNO', 'ROA', 'ROC', 'ROW', 'RST', 'RSW', 'SAF', 'SAN', 'SAT', 'SAV', 'SBA', 'SBN', 'SBP', 'SCC', 'SCE', 'SCK', 'SDF', 'SEA', 'SFB', 'SFO', 'SGF', 'SGU', 'SHR', 'SHV', 'SIT', 'SJC', 'SJT', 'SJU', 'SLC', 'SLN', 'SMF', 'SMX', 'SNA', 'SPI', 'SPN', 'SPS', 'SRQ', 'STC', 'STL', 'STS', 'STT', 'STX', 'SUN', 'SUX', 'SWF', 'SWO', 'SYR', 'TLH', 'TOL', 'TPA', 'TRI', 'TTN', 'TUL', 'TUS', 'TVC', 'TWF', 'TXK', 'TYR', 'TYS', 'USA', 'VCT', 'VPS', 'WRG', 'WYS', 'XNA', 'XWA', 'YAK', 'YUM']\n",
      "üíæ Saved all airports to: data/processed/origin_airports.csv\n",
      "\n",
      "üìà Coverage Analysis:\n",
      "   Top 10 airports: 33.9% of flights\n",
      "   Top 25 airports: 59.2% of flights\n",
      "   Top 50 airports: 78.3% of flights\n",
      "   Top 100 airports: 91.2% of flights\n"
     ]
    }
   ],
   "source": [
    "# Get airport statistics (origin + destination combined)\n",
    "origin_counts = flights_df['ORIGIN'].value_counts()\n",
    "dest_counts = flights_df['DEST'].value_counts()\n",
    "\n",
    "# Combine and sum (airports appear in both origin and dest)\n",
    "airport_counts = pd.concat([origin_counts, dest_counts]).groupby(level=0).sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AIRPORT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total unique airports: {len(airport_counts)}\")\n",
    "print(f\"Total flights: {airport_counts.sum():,}\")\n",
    "\n",
    "# Top 50 airports\n",
    "top_50 = airport_counts.head(50)\n",
    "print(f\"\\nüìä Top 50 Airports (by flight volume):\")\n",
    "print(f\"   Total flights in top 50: {top_50.sum():,} ({top_50.sum()/airport_counts.sum()*100:.1f}% of all flights)\")\n",
    "print(f\"\\nTop 50 airports:\")\n",
    "for i, (airport, count) in enumerate(top_50.items(), 1):\n",
    "    pct = count / airport_counts.sum() * 100\n",
    "    print(f\"   {i:2d}. {airport}: {count:,} flights ({pct:.2f}%)\")\n",
    "\n",
    "# Save top 50\n",
    "top_50.to_csv(\"data/processed/top_50_airports.csv\", header=['Flight_Count'])\n",
    "print(f\"\\nüíæ Saved top 50 to: data/processed/top_50_airports.csv\")\n",
    "\n",
    "# All airports (sorted by code)\n",
    "all_airports = sorted(airport_counts.index.tolist())\n",
    "print(f\"\\nüìã All airports ({len(all_airports)} total, sorted by code):\")\n",
    "print(all_airports)\n",
    "\n",
    "# Save all airports\n",
    "airport_counts.to_csv(\"data/processed/origin_airports.csv\", header=['Flight_Count'])\n",
    "print(f\"üíæ Saved all airports to: data/processed/origin_airports.csv\")\n",
    "\n",
    "# Coverage analysis\n",
    "print(f\"\\nüìà Coverage Analysis:\")\n",
    "print(f\"   Top 10 airports: {airport_counts.head(10).sum()/airport_counts.sum()*100:.1f}% of flights\")\n",
    "print(f\"   Top 25 airports: {airport_counts.head(25).sum()/airport_counts.sum()*100:.1f}% of flights\")\n",
    "print(f\"   Top 50 airports: {airport_counts.head(50).sum()/airport_counts.sum()*100:.1f}% of flights\")\n",
    "print(f\"   Top 100 airports: {airport_counts.head(100).sum()/airport_counts.sum()*100:.1f}% of flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b7b75",
   "metadata": {},
   "source": [
    "## Import and Merge NOAA data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1120a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FETCHING COORDINATES FOR TOP 50 AIRPORTS\n",
      "============================================================\n",
      "üìä Top 50 airports to fetch:\n",
      "   ORD, DEN, DFW, ATL, CLT, LAX, PHX, LAS, SEA, MCO, DCA, SFO, BOS, LGA, EWR, DTW, IAH, SLC, MSP, MIA, JFK, BNA, BWI, PHL, SAN, AUS, FLL, MDW, TPA, DAL, STL, PDX, HNL, SMF, IAD, HOU, RDU, MSY, MCI, SJC, IND, SNA, CMH, PIT, SAT, CLE, OAK, SJU, CVG, RSW\n",
      "\n",
      "   Total flights in top 50: 2,714,667\n",
      "   Coverage: 78.3% of all flights\n",
      "üåç Fetching coordinates for 50 airports...\n",
      "   Progress: 10/50 airports\n",
      "   Progress: 20/50 airports\n",
      "   Progress: 30/50 airports\n",
      "   Progress: 40/50 airports\n",
      "   Progress: 50/50 airports\n",
      "\n",
      "‚úÖ Found coordinates for 50/50 airports\n",
      "\n",
      "üíæ Saved to: data/processed/airport_coordinates_top50.csv\n",
      "\n",
      "üìã Coordinates fetched:\n",
      "  airport_code        lat         lon\n",
      "0          ORD  41.978252  -87.909235\n",
      "1          DEN  39.860668 -104.685367\n",
      "2          DFW  32.896519  -97.046522\n",
      "3          ATL  33.637401  -84.429816\n",
      "4          CLT  35.210741  -80.945744\n",
      "5          LAX  33.942167 -118.421359\n",
      "6          PHX  33.432849 -112.006792\n",
      "7          LAS  36.086103 -115.161100\n",
      "8          SEA  47.447567 -122.308016\n",
      "9          MCO  28.412904  -81.309443\n"
     ]
    }
   ],
   "source": [
    "def get_airport_coordinates_geocoding(airports_list):\n",
    "    \"\"\"\n",
    "    Get airport coordinates using Nominatim (OpenStreetMap) - free, no API key.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    coords = []\n",
    "    failed = []\n",
    "    \n",
    "    print(f\"üåç Fetching coordinates for {len(airports_list)} airports...\")\n",
    "    \n",
    "    for i, airport in enumerate(airports_list, 1):\n",
    "        try:\n",
    "            # Try airport code search\n",
    "            url = \"https://nominatim.openstreetmap.org/search\"\n",
    "            params = {\n",
    "                'q': f\"{airport} airport USA\",\n",
    "                'format': 'json',\n",
    "                'limit': 1\n",
    "            }\n",
    "            headers = {'User-Agent': 'Flight-Cancellation-Project-1.0'}  # Required by Nominatim\n",
    "            \n",
    "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if data and len(data) > 0:\n",
    "                coords.append({\n",
    "                    'airport_code': airport,\n",
    "                    'lat': float(data[0]['lat']),\n",
    "                    'lon': float(data[0]['lon']),\n",
    "                    'airport_name': data[0].get('display_name', '')\n",
    "                })\n",
    "                if i % 10 == 0:  # Progress update every 10 (since only 50 total)\n",
    "                    print(f\"   Progress: {i}/{len(airports_list)} airports\")\n",
    "            else:\n",
    "                failed.append(airport)\n",
    "            \n",
    "            time.sleep(1)  # Rate limiting: 1 request per second (free tier)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error for {airport}: {e}\")\n",
    "            failed.append(airport)\n",
    "    \n",
    "    df = pd.DataFrame(coords)\n",
    "    print(f\"\\n‚úÖ Found coordinates for {len(df)}/{len(airports_list)} airports\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"‚ö†Ô∏è Failed to find: {len(failed)} airports\")\n",
    "        print(f\"   Failed airports: {failed}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load top 50 airports from CSV\n",
    "top_50_df = pd.read_csv(\"data/processed/top_50_airports.csv\", index_col=0)\n",
    "top_50_airports = top_50_df.index.tolist()  # Get airport codes (index)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FETCHING COORDINATES FOR TOP 50 AIRPORTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Top 50 airports to fetch:\")\n",
    "print(f\"   {', '.join(top_50_airports)}\")\n",
    "print(f\"\\n   Total flights in top 50: {top_50_df['Flight_Count'].sum():,}\")\n",
    "print(f\"   Coverage: {top_50_df['Flight_Count'].sum() / (flights_df['ORIGIN'].value_counts().sum() + flights_df['DEST'].value_counts().sum()) * 100:.1f}% of all flights\")\n",
    "\n",
    "# Get coordinates (this will take ~50 seconds for 50 airports)\n",
    "airport_coords_top50 = get_airport_coordinates_geocoding(top_50_airports)\n",
    "\n",
    "# Save\n",
    "airport_coords_top50.to_csv(\"data/processed/airport_coordinates_top50.csv\", index=False)\n",
    "print(f\"\\nüíæ Saved to: data/processed/airport_coordinates_top50.csv\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìã Coordinates fetched:\")\n",
    "print(airport_coords_top50[['airport_code', 'lat', 'lon']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data_top50(flights_df, airport_coords_df, api_key, output_path=None):\n",
    "    \"\"\"\n",
    "    Fetch weather data for top 50 airports only and merge with flights.\n",
    "    \n",
    "    Args:\n",
    "        flights_df: DataFrame with flight data (must have ORIGIN, DEST, FL_DATE)\n",
    "        airport_coords_df: DataFrame with top 50 airport coordinates (airport_code, lat, lon)\n",
    "        api_key: Visual Crossing API key\n",
    "        output_path: Optional path to save merged data\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with weather data\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FETCHING WEATHER DATA (TOP 50 AIRPORTS ONLY)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get list of top 50 airports from coordinates\n",
    "    top_50_airports = set(airport_coords_df['airport_code'].tolist())\n",
    "    print(f\"\\nüìä Top 50 airports: {len(top_50_airports)} airports\")\n",
    "    \n",
    "    # Filter flights to only include top 50 airports\n",
    "    print(\"\\nüîç Filtering flights to top 50 airports...\")\n",
    "    flights_filtered = flights_df[\n",
    "        flights_df['ORIGIN'].isin(top_50_airports) & \n",
    "        flights_df['DEST'].isin(top_50_airports)\n",
    "    ].copy()\n",
    "    \n",
    "    original_count = len(flights_df)\n",
    "    filtered_count = len(flights_filtered)\n",
    "    coverage = filtered_count / original_count * 100\n",
    "    \n",
    "    print(f\"   Original flights: {original_count:,}\")\n",
    "    print(f\"   Filtered flights: {filtered_count:,} ({coverage:.1f}% of total)\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(\"‚ùå No flights found with top 50 airports!\")\n",
    "        return flights_df\n",
    "    \n",
    "    # Create airport coordinate lookup\n",
    "    coord_lookup = airport_coords_df.set_index('airport_code')[['lat', 'lon']].to_dict('index')\n",
    "    \n",
    "    # Get unique airport-date combinations (for both origin and dest)\n",
    "    print(\"\\nüìä Analyzing flight data...\")\n",
    "    \n",
    "    # Origin airports\n",
    "    origin_dates = flights_filtered[['ORIGIN', 'FL_DATE']].drop_duplicates()\n",
    "    origin_dates.columns = ['airport', 'date']\n",
    "    \n",
    "    # Destination airports\n",
    "    dest_dates = flights_filtered[['DEST', 'FL_DATE']].drop_duplicates()\n",
    "    dest_dates.columns = ['airport', 'date']\n",
    "    \n",
    "    # Combine and get unique combinations\n",
    "    all_airport_dates = pd.concat([origin_dates, dest_dates]).drop_duplicates()\n",
    "    all_airport_dates['date'] = pd.to_datetime(all_airport_dates['date']).dt.date\n",
    "    \n",
    "    print(f\"   Unique airport-date combinations: {len(all_airport_dates):,}\")\n",
    "    print(f\"   (Much faster than {len(flights_df[['ORIGIN', 'FL_DATE']].drop_duplicates()) + len(flights_df[['DEST', 'FL_DATE']].drop_duplicates()):,} for all airports!)\")\n",
    "    \n",
    "    # Fetch weather data\n",
    "    weather_data = []\n",
    "    failed = []\n",
    "    consecutive_429s = 0\n",
    "    \n",
    "    for idx, row in all_airport_dates.iterrows():\n",
    "        airport = row['airport']\n",
    "        date = row['date']\n",
    "        \n",
    "        # Get coordinates (should always exist since we filtered)\n",
    "        if airport not in coord_lookup:\n",
    "            failed.append((airport, date, \"No coordinates\"))\n",
    "            continue\n",
    "        \n",
    "        lat = coord_lookup[airport]['lat']\n",
    "        lon = coord_lookup[airport]['lon']\n",
    "        \n",
    "        retries = 0\n",
    "        max_retries = 3\n",
    "        \n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                # Visual Crossing API call\n",
    "                url = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n",
    "                params = {\n",
    "                    'location': f\"{lat},{lon}\",\n",
    "                    'date': date.strftime('%Y-%m-%d'),\n",
    "                    'key': api_key,\n",
    "                    'include': 'days',\n",
    "                    'elements': 'datetime,temp,precip,windspeed,visibility,snow,conditions'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params, headers={'User-Agent': 'Flight-Cancellation-Project'}, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    if 'days' in data and len(data['days']) > 0:\n",
    "                        day_data = data['days'][0]\n",
    "                        weather_data.append({\n",
    "                            'airport': airport,\n",
    "                            'date': date,\n",
    "                            'temp_max': day_data.get('tempmax'),\n",
    "                            'temp_min': day_data.get('tempmin'),\n",
    "                            'temp_avg': day_data.get('temp'),\n",
    "                            'precip': day_data.get('precip', 0),\n",
    "                            'snow': day_data.get('snow', 0),\n",
    "                            'windspeed': day_data.get('windspeed'),\n",
    "                            'visibility': day_data.get('visibility'),\n",
    "                            'conditions': day_data.get('conditions', '')\n",
    "                        })\n",
    "                        consecutive_429s = 0  # Reset on success\n",
    "                    \n",
    "                    break  # Success, exit retry loop\n",
    "                \n",
    "                elif response.status_code == 429:\n",
    "                    consecutive_429s += 1\n",
    "                    wait_time = min(60 * consecutive_429s, 300)  # Max 5 min wait\n",
    "                    print(f\"   ‚ö†Ô∏è Rate limit (429). Waiting {wait_time} seconds... (attempt {retries+1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                    retries += 1\n",
    "                    \n",
    "                    if retries >= max_retries:\n",
    "                        print(f\"   ‚ùå Max retries reached for {airport} on {date}\")\n",
    "                        failed.append((airport, date, \"Rate limit exceeded\"))\n",
    "                        break\n",
    "                \n",
    "                elif response.status_code == 401:\n",
    "                    print(\"‚ùå API key invalid!\")\n",
    "                    return None\n",
    "                \n",
    "                else:\n",
    "                    failed.append((airport, date, f\"Status {response.status_code}\"))\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    consecutive_429s += 1\n",
    "                    wait_time = min(60 * consecutive_429s, 300)\n",
    "                    print(f\"   ‚ö†Ô∏è Rate limit error. Waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Error for {airport} on {date}: {e}\")\n",
    "                    failed.append((airport, date, str(e)))\n",
    "                    break\n",
    "        \n",
    "        # Progress update\n",
    "        if len(weather_data) % 50 == 0:\n",
    "            print(f\"   Progress: {len(weather_data)}/{len(all_airport_dates)} fetched\")\n",
    "        \n",
    "        # Rate limiting: 2 seconds between requests (more conservative)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Create weather DataFrame\n",
    "    weather_df = pd.DataFrame(weather_data)\n",
    "    \n",
    "    if len(weather_df) > 0:\n",
    "        weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "        print(f\"\\n‚úÖ Fetched weather for {len(weather_df):,} airport-date combinations\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No weather data fetched!\")\n",
    "        return flights_filtered\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"‚ö†Ô∏è Failed to fetch: {len(failed)} combinations\")\n",
    "        if len(failed) <= 10:\n",
    "            print(f\"   Failed: {failed}\")\n",
    "    \n",
    "    # Merge with flight data (use filtered flights)\n",
    "    print(\"\\nüîó Merging weather data with flights...\")\n",
    "    \n",
    "    # Merge origin weather\n",
    "    flights_merged = flights_filtered.merge(\n",
    "        weather_df,\n",
    "        left_on=['ORIGIN', 'FL_DATE'],\n",
    "        right_on=['airport', 'date'],\n",
    "        how='left',\n",
    "        suffixes=('', '_origin')\n",
    "    )\n",
    "    \n",
    "    # Rename origin weather columns\n",
    "    origin_cols = ['temp_max', 'temp_min', 'temp_avg', 'precip', 'snow', 'windspeed', 'visibility', 'conditions']\n",
    "    rename_dict = {col: f'{col}_origin' for col in origin_cols if col in flights_merged.columns}\n",
    "    flights_merged = flights_merged.rename(columns=rename_dict)\n",
    "    \n",
    "    # Drop merge helper columns\n",
    "    flights_merged = flights_merged.drop(columns=['airport', 'date'], errors='ignore')\n",
    "    \n",
    "    # Merge destination weather\n",
    "    flights_merged = flights_merged.merge(\n",
    "        weather_df,\n",
    "        left_on=['DEST', 'FL_DATE'],\n",
    "        right_on=['airport', 'date'],\n",
    "        how='left',\n",
    "        suffixes=('', '_dest')\n",
    "    )\n",
    "    \n",
    "    # Rename destination weather columns\n",
    "    rename_dict = {col: f'{col}_dest' for col in origin_cols if col in flights_merged.columns}\n",
    "    flights_merged = flights_merged.rename(columns=rename_dict)\n",
    "    \n",
    "    # Drop merge helper columns\n",
    "    flights_merged = flights_merged.drop(columns=['airport', 'date'], errors='ignore')\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n‚úÖ Merged weather data\")\n",
    "    print(f\"   Total flights (filtered): {len(flights_merged):,}\")\n",
    "    print(f\"   Flights with origin weather: {flights_merged['temp_avg_origin'].notna().sum():,} ({flights_merged['temp_avg_origin'].notna().mean()*100:.1f}%)\")\n",
    "    print(f\"   Flights with dest weather: {flights_merged['temp_avg_dest'].notna().sum():,} ({flights_merged['temp_avg_dest'].notna().mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Save if output path provided\n",
    "    if output_path:\n",
    "        flights_merged.to_csv(output_path, index=False)\n",
    "        print(f\"\\nüíæ Saved merged data to: {output_path}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return flights_merged\n",
    "\n",
    "# Usage:\n",
    "# 1. Load your data\n",
    "airport_coords_top50 = pd.read_csv(\"data/processed/airport_coordinates_top50.csv\")\n",
    "flights_df = pd.read_csv(\"data/processed/flights_combined.csv\")\n",
    "flights_df['FL_DATE'] = pd.to_datetime(flights_df['FL_DATE'])\n",
    "\n",
    "# 2. Set your Visual Crossing API key\n",
    "api_key = \"G2SRNMYAF44NW4YYGMFXKK9UV\"  # Replace with your actual key\n",
    "\n",
    "# 3. Fetch and merge weather data (only for top 50 airports)\n",
    "flights_with_weather = fetch_weather_data_top50(\n",
    "    flights_df,\n",
    "    airport_coords_top50,\n",
    "    api_key,\n",
    "    output_path=\"data/processed/flights_with_weather_top50.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4296df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FETCHING WEATHER FROM OPEN-METEO (FREE)\n",
      "============================================================\n",
      "üìä Fetching weather for 4,600 airport-date combinations\n"
     ]
    }
   ],
   "source": [
    "def fetch_weather_openmeteo_complete(flights_df, airport_coords_df, output_path=None):\n",
    "    \"\"\"\n",
    "    Complete weather fetching using Open-Meteo (FREE, no API key).\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FETCHING WEATHER FROM OPEN-METEO (FREE)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter to top 50 airports\n",
    "    top_50_airports = set(airport_coords_df['airport_code'].tolist())\n",
    "    flights_filtered = flights_df[\n",
    "        flights_df['ORIGIN'].isin(top_50_airports) & \n",
    "        flights_df['DEST'].isin(top_50_airports)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get unique combinations\n",
    "    origin_dates = flights_filtered[['ORIGIN', 'FL_DATE']].drop_duplicates()\n",
    "    dest_dates = flights_filtered[['DEST', 'FL_DATE']].drop_duplicates()\n",
    "    all_airport_dates = pd.concat([\n",
    "        origin_dates.rename(columns={'ORIGIN': 'airport', 'FL_DATE': 'date'}),\n",
    "        dest_dates.rename(columns={'DEST': 'airport', 'FL_DATE': 'date'})\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    coord_lookup = airport_coords_df.set_index('airport_code')[['lat', 'lon']].to_dict('index')\n",
    "    \n",
    "    print(f\"üìä Fetching weather for {len(all_airport_dates):,} airport-date combinations\")\n",
    "    \n",
    "    weather_data = []\n",
    "    \n",
    "    for idx, row in all_airport_dates.iterrows():\n",
    "        airport = row['airport']\n",
    "        date = pd.to_datetime(row['date'])\n",
    "        \n",
    "        if airport not in coord_lookup:\n",
    "            continue\n",
    "        \n",
    "        lat = coord_lookup[airport]['lat']\n",
    "        lon = coord_lookup[airport]['lon']\n",
    "        \n",
    "        try:\n",
    "            url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "            params = {\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'start_date': date.strftime('%Y-%m-%d'),\n",
    "                'end_date': date.strftime('%Y-%m-%d'),\n",
    "                'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum,windspeed_10m_max,windgusts_10m_max',\n",
    "                'timezone': 'auto'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'daily' in data and data['daily']:\n",
    "                    daily = data['daily']\n",
    "                    temp_max = daily.get('temperature_2m_max', [None])[0]\n",
    "                    temp_min = daily.get('temperature_2m_min', [None])[0]\n",
    "                    \n",
    "                    weather_data.append({\n",
    "                        'airport': airport,\n",
    "                        'date': date.date(),\n",
    "                        'temp_max': temp_max,\n",
    "                        'temp_min': temp_min,\n",
    "                        'temp_avg': (temp_max + temp_min) / 2 if temp_max and temp_min else None,\n",
    "                        'precip': daily.get('precipitation_sum', [0])[0] or 0,\n",
    "                        'windspeed': daily.get('windspeed_10m_max', [None])[0],\n",
    "                        'windgust': daily.get('windgusts_10m_max', [None])[0],\n",
    "                    })\n",
    "            \n",
    "            if len(weather_data) % 50 == 0:\n",
    "                print(f\"   Progress: {len(weather_data)}/{len(all_airport_dates)}\")\n",
    "            \n",
    "            time.sleep(0.5)  # 2 requests/sec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error for {airport}: {e}\")\n",
    "    \n",
    "    weather_df = pd.DataFrame(weather_data)\n",
    "    if len(weather_df) > 0:\n",
    "        weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "    \n",
    "    # Merge with flights (same as before)\n",
    "    # ... (merge logic)\n",
    "    \n",
    "    return flights_merged\n",
    "\n",
    "# Usage (NO API KEY NEEDED!):\n",
    "flights_with_weather = fetch_weather_openmeteo_complete(\n",
    "    flights_df,\n",
    "    airport_coords_top50,\n",
    "    output_path=\"data/processed/flights_with_weather_openmeteo.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab213abf",
   "metadata": {},
   "source": [
    "## Phase 4: Data Preprocessing & Feature Engineering\n",
    "\n",
    "**Goal**: Clean data, create features (time-based, weather, airport, airline, route)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995037d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f2fb28",
   "metadata": {},
   "source": [
    "## Phase 5: Baseline Models\n",
    "\n",
    "**Goal**: Build simple prediction models (Logistic Regression, XGBoost) to establish baseline performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a14153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bf400",
   "metadata": {},
   "source": [
    "## Phase 6: Time Series Models\n",
    "\n",
    "**Goal**: Use transformers (PatchTST/TST) to capture temporal patterns in flight cancellations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa78ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series transformer models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9edaa6",
   "metadata": {},
   "source": [
    "## Phase 7: Causal Inference Setup\n",
    "\n",
    "**Goal**: Identify causal relationships - separate weather effects from operational effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal inference (difference-in-differences, propensity score matching)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365efe06",
   "metadata": {},
   "source": [
    "## Phase 8: Attribution Model\n",
    "\n",
    "**Goal**: Attribute each cancellation to specific causes (weather vs. mechanical vs. crew vs. other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution using Shapley values, counterfactual analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c33e0",
   "metadata": {},
   "source": [
    "## Phase 9: Integration & Evaluation\n",
    "\n",
    "**Goal**: Combine prediction + attribution, evaluate end-to-end system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfd745",
   "metadata": {},
   "source": [
    "## Phase 10: Results & Insights\n",
    "\n",
    "**Goal**: Summarize findings, visualize results, document key insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results, visualizations, and conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dec233",
   "metadata": {},
   "source": [
    "## Phase 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Goal**: Understand data distributions, cancellation patterns, correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e53a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf283059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlprojects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
